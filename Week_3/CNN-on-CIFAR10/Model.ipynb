{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "8MOyDbjKhA5H"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader, random_split, Subset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as func"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converts the image from [H,W,C] to a PyTorch tensor [C,H,W] --> what the model will expect\n",
        "base_transform = transforms.ToTensor()\n",
        "full_train = CIFAR10(root='./data', train=True, download=True)\n",
        "full_test = CIFAR10(root='./data', train=False, download=True)\n",
        "train_subset = Subset(full_train, range(50000))\n",
        "test_subset = Subset(full_test, range(10000))\n",
        "\n",
        "# For Normalisation, I used the values based on this repo : https://github.com/kuangliu/pytorch-cifar/issues/19\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomResizedCrop(32),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
        "                         std=[0.2023, 0.1994, 0.2010])\n",
        "])\n",
        "\n",
        "# No data augmentation in test set as it will show the the model works on real world, unseen data\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
        "                         std=[0.2023, 0.1994, 0.2010])\n",
        "])\n",
        "\n",
        "train_subset.dataset.transform = train_transform\n",
        "test_subset.dataset.transform = test_transform\n",
        "train_loader = DataLoader(train_subset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_subset, batch_size=128, shuffle=False)"
      ],
      "metadata": {
        "id": "o5Ave1t_h37O"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, to design the CNN model, I noted the following :\n",
        "# Start with fewer channels to learn basic edges and then increases to capture more complex features\n",
        "# Two convolutional kernels of 3 x 3 should capture more precise patterns than one large kernel\n",
        "# Perceptron dropout towards the end to prevent overfitting\n",
        "\n",
        "class Trial1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Trial1, self).__init__()\n",
        "\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128 * 4 * 4, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "nn = Trial1()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "nn = nn.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(nn.parameters(), lr=0.01)\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    nn.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = nn(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    train_accuracy = 100 * correct / total\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}, Accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "nn.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = nn(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "test_accuracy = 100 * correct / total\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K59vyRbhprby",
        "outputId": "a3cea57d-7cde-4168-88fb-6018d4c6a90d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 2.3381, Accuracy: 18.20%\n",
            "Epoch [2/10], Loss: 2.0280, Accuracy: 22.58%\n",
            "Epoch [3/10], Loss: 2.0150, Accuracy: 23.45%\n",
            "Epoch [4/10], Loss: 2.0059, Accuracy: 23.79%\n",
            "Epoch [5/10], Loss: 2.0023, Accuracy: 24.21%\n",
            "Epoch [6/10], Loss: 2.0176, Accuracy: 23.64%\n",
            "Epoch [7/10], Loss: 2.0094, Accuracy: 23.75%\n",
            "Epoch [8/10], Loss: 2.0222, Accuracy: 23.42%\n",
            "Epoch [9/10], Loss: 2.0428, Accuracy: 22.90%\n",
            "Epoch [10/10], Loss: 2.0738, Accuracy: 21.74%\n",
            "Test Accuracy: 21.73%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Trial2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(3, 10, 3)\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Linear(10, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc(x)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "nn = Trial2().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(nn.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(10):\n",
        "    nn.train()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = nn(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    print(f\"Epoch {epoch+1}: loss = {running_loss:.4f}, acc = {100 * correct / total:.2f}%\")\n",
        "\n",
        "nn.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = nn(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIgBMre52Txv",
        "outputId": "8bcc3dbc-c59c-4e3c-d440-d31850ec2a8a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: loss = 851.3347, acc = 18.78%\n",
            "Epoch 2: loss = 804.6560, acc = 23.07%\n",
            "Epoch 3: loss = 792.7060, acc = 24.68%\n",
            "Epoch 4: loss = 785.9583, acc = 25.54%\n",
            "Epoch 5: loss = 778.8800, acc = 26.05%\n",
            "Epoch 6: loss = 772.7614, acc = 26.66%\n",
            "Epoch 7: loss = 766.6993, acc = 27.23%\n",
            "Epoch 8: loss = 760.3260, acc = 27.87%\n",
            "Epoch 9: loss = 755.9728, acc = 28.25%\n",
            "Epoch 10: loss = 751.1162, acc = 28.60%\n",
            "Test Accuracy: 30.79%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implemented based on the repo : https://github.com/fares-ds/Cifar-10_Image_Classification_Using_CNNs/blob/master/Cifar_10_image_classification_using_cnn.ipynb\n",
        "# The original code was written using the Tensorflow library, I attempted to understand the architecture and then replicate it in Pytorch\n",
        "\n",
        "class Trial3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Trial3, self).__init__()\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Trial3().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    train_accuracy = 100 * correct / total\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}, Accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "test_accuracy = 100 * correct / total\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnLhMComGrBL",
        "outputId": "9ea64f30-d045-45b5-97db-17f0138b6356"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 1.8567, Accuracy: 31.04%\n",
            "Epoch [2/100], Loss: 1.6153, Accuracy: 40.80%\n",
            "Epoch [3/100], Loss: 1.5015, Accuracy: 45.27%\n",
            "Epoch [4/100], Loss: 1.4235, Accuracy: 48.57%\n",
            "Epoch [5/100], Loss: 1.3711, Accuracy: 51.08%\n",
            "Epoch [6/100], Loss: 1.3172, Accuracy: 53.13%\n",
            "Epoch [7/100], Loss: 1.2766, Accuracy: 54.45%\n",
            "Epoch [8/100], Loss: 1.2394, Accuracy: 55.97%\n",
            "Epoch [9/100], Loss: 1.2114, Accuracy: 57.17%\n",
            "Epoch [10/100], Loss: 1.1824, Accuracy: 58.34%\n",
            "Epoch [11/100], Loss: 1.1618, Accuracy: 58.79%\n",
            "Epoch [12/100], Loss: 1.1428, Accuracy: 59.91%\n",
            "Epoch [13/100], Loss: 1.1178, Accuracy: 60.54%\n",
            "Epoch [14/100], Loss: 1.0921, Accuracy: 61.39%\n",
            "Epoch [15/100], Loss: 1.0781, Accuracy: 62.16%\n",
            "Epoch [16/100], Loss: 1.0667, Accuracy: 62.60%\n",
            "Epoch [17/100], Loss: 1.0547, Accuracy: 63.29%\n",
            "Epoch [18/100], Loss: 1.0347, Accuracy: 64.02%\n",
            "Epoch [19/100], Loss: 1.0357, Accuracy: 63.79%\n",
            "Epoch [20/100], Loss: 1.0164, Accuracy: 64.84%\n",
            "Epoch [21/100], Loss: 1.0089, Accuracy: 64.69%\n",
            "Epoch [22/100], Loss: 1.0034, Accuracy: 64.98%\n",
            "Epoch [23/100], Loss: 0.9854, Accuracy: 65.91%\n",
            "Epoch [24/100], Loss: 0.9879, Accuracy: 65.58%\n",
            "Epoch [25/100], Loss: 0.9672, Accuracy: 66.29%\n",
            "Epoch [26/100], Loss: 0.9623, Accuracy: 66.71%\n",
            "Epoch [27/100], Loss: 0.9566, Accuracy: 66.50%\n",
            "Epoch [28/100], Loss: 0.9513, Accuracy: 66.79%\n",
            "Epoch [29/100], Loss: 0.9408, Accuracy: 67.24%\n",
            "Epoch [30/100], Loss: 0.9435, Accuracy: 67.18%\n",
            "Epoch [31/100], Loss: 0.9345, Accuracy: 67.58%\n",
            "Epoch [32/100], Loss: 0.9299, Accuracy: 67.76%\n",
            "Epoch [33/100], Loss: 0.9148, Accuracy: 68.21%\n",
            "Epoch [34/100], Loss: 0.9084, Accuracy: 68.53%\n",
            "Epoch [35/100], Loss: 0.9126, Accuracy: 68.44%\n",
            "Epoch [36/100], Loss: 0.9010, Accuracy: 68.96%\n",
            "Epoch [37/100], Loss: 0.9070, Accuracy: 68.68%\n",
            "Epoch [38/100], Loss: 0.9014, Accuracy: 68.84%\n",
            "Epoch [39/100], Loss: 0.8971, Accuracy: 68.94%\n",
            "Epoch [40/100], Loss: 0.8899, Accuracy: 69.06%\n",
            "Epoch [41/100], Loss: 0.8887, Accuracy: 69.38%\n",
            "Epoch [42/100], Loss: 0.8818, Accuracy: 69.55%\n",
            "Epoch [43/100], Loss: 0.8780, Accuracy: 69.67%\n",
            "Epoch [44/100], Loss: 0.8769, Accuracy: 69.60%\n",
            "Epoch [45/100], Loss: 0.8722, Accuracy: 70.02%\n",
            "Epoch [46/100], Loss: 0.8694, Accuracy: 69.92%\n",
            "Epoch [47/100], Loss: 0.8653, Accuracy: 69.99%\n",
            "Epoch [48/100], Loss: 0.8612, Accuracy: 69.98%\n",
            "Epoch [49/100], Loss: 0.8547, Accuracy: 70.44%\n",
            "Epoch [50/100], Loss: 0.8554, Accuracy: 70.38%\n",
            "Epoch [51/100], Loss: 0.8568, Accuracy: 70.23%\n",
            "Epoch [52/100], Loss: 0.8513, Accuracy: 70.41%\n",
            "Epoch [53/100], Loss: 0.8493, Accuracy: 70.63%\n",
            "Epoch [54/100], Loss: 0.8442, Accuracy: 70.75%\n",
            "Epoch [55/100], Loss: 0.8421, Accuracy: 70.67%\n",
            "Epoch [56/100], Loss: 0.8360, Accuracy: 70.95%\n",
            "Epoch [57/100], Loss: 0.8333, Accuracy: 70.99%\n",
            "Epoch [58/100], Loss: 0.8370, Accuracy: 70.92%\n",
            "Epoch [59/100], Loss: 0.8299, Accuracy: 71.16%\n",
            "Epoch [60/100], Loss: 0.8273, Accuracy: 71.32%\n",
            "Epoch [61/100], Loss: 0.8236, Accuracy: 71.58%\n",
            "Epoch [62/100], Loss: 0.8242, Accuracy: 71.22%\n",
            "Epoch [63/100], Loss: 0.8191, Accuracy: 71.63%\n",
            "Epoch [64/100], Loss: 0.8196, Accuracy: 71.58%\n",
            "Epoch [65/100], Loss: 0.8205, Accuracy: 71.49%\n",
            "Epoch [66/100], Loss: 0.8153, Accuracy: 71.71%\n",
            "Epoch [67/100], Loss: 0.8124, Accuracy: 71.97%\n",
            "Epoch [68/100], Loss: 0.8208, Accuracy: 71.56%\n",
            "Epoch [69/100], Loss: 0.8066, Accuracy: 72.03%\n",
            "Epoch [70/100], Loss: 0.8050, Accuracy: 72.06%\n",
            "Epoch [71/100], Loss: 0.8033, Accuracy: 72.01%\n",
            "Epoch [72/100], Loss: 0.8022, Accuracy: 72.20%\n",
            "Epoch [73/100], Loss: 0.8064, Accuracy: 72.02%\n",
            "Epoch [74/100], Loss: 0.8023, Accuracy: 72.08%\n",
            "Epoch [75/100], Loss: 0.7958, Accuracy: 72.26%\n",
            "Epoch [76/100], Loss: 0.7953, Accuracy: 72.40%\n",
            "Epoch [77/100], Loss: 0.7960, Accuracy: 72.39%\n",
            "Epoch [78/100], Loss: 0.7952, Accuracy: 72.31%\n",
            "Epoch [79/100], Loss: 0.7912, Accuracy: 72.45%\n",
            "Epoch [80/100], Loss: 0.7913, Accuracy: 72.59%\n",
            "Epoch [81/100], Loss: 0.7919, Accuracy: 72.69%\n",
            "Epoch [82/100], Loss: 0.7902, Accuracy: 72.70%\n",
            "Epoch [83/100], Loss: 0.7832, Accuracy: 72.63%\n",
            "Epoch [84/100], Loss: 0.7869, Accuracy: 72.71%\n",
            "Epoch [85/100], Loss: 0.7800, Accuracy: 73.07%\n",
            "Epoch [86/100], Loss: 0.7830, Accuracy: 72.88%\n",
            "Epoch [87/100], Loss: 0.7780, Accuracy: 73.18%\n",
            "Epoch [88/100], Loss: 0.7815, Accuracy: 72.95%\n",
            "Epoch [89/100], Loss: 0.7774, Accuracy: 73.08%\n",
            "Epoch [90/100], Loss: 0.7710, Accuracy: 73.36%\n",
            "Epoch [91/100], Loss: 0.7712, Accuracy: 73.28%\n",
            "Epoch [92/100], Loss: 0.7676, Accuracy: 73.38%\n",
            "Epoch [93/100], Loss: 0.7665, Accuracy: 73.39%\n",
            "Epoch [94/100], Loss: 0.7648, Accuracy: 73.46%\n",
            "Epoch [95/100], Loss: 0.7731, Accuracy: 73.28%\n",
            "Epoch [96/100], Loss: 0.7774, Accuracy: 73.11%\n",
            "Epoch [97/100], Loss: 0.7633, Accuracy: 73.49%\n",
            "Epoch [98/100], Loss: 0.7740, Accuracy: 73.29%\n",
            "Epoch [99/100], Loss: 0.7601, Accuracy: 73.70%\n",
            "Epoch [100/100], Loss: 0.7656, Accuracy: 73.41%\n",
            "Test Accuracy: 85.94%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tZZISYNXeG7r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}